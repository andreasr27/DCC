{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f27833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd1bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from utils import *\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802514a",
   "metadata": {},
   "source": [
    "### Hyperparameter of implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14472352",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_probability_numerator = 20\n",
    "notifications = 2\n",
    "epsilon = 0.2\n",
    "deletion_prob_at_each_step = 0.2\n",
    "num_samples_for_agreement_and_heavyness_calculation = 2\n",
    "num_samples_for_connect_procedure = 2\n",
    "num_experiments = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61134a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn color palette for color-blind friendly colors\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305c8e4f",
   "metadata": {},
   "source": [
    "#### The list datasets contains the relative location of each dataset. Each line of the dataset contains two comma separated numbers which correspond to an edge between two nodes, e.g., \"84424, 276\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "660a1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets contains the relative location of the datasets. Each line of the dataset contains \n",
    "datasets =  [\"datasets/musae-facebook.csv\",  \"datasets/Email-Enron.csv\", \"datasets/Cit-HepTh.csv\", \"datasets/CA-AstroPh.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ab5bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agreement = 0.9691669154622036 vs dynamic pivot = 1.120703886478987 vs singleton = 1.0 vs classical pivot = 1.0929909906745578\n",
      "agreement = 0.9536857222122493 vs dynamic pivot = 1.01882163508875 vs singleton = 1.0 vs classical pivot = 1.0721097094614074\n",
      "agreement = 0.9977291049267069 vs dynamic pivot = 1.1944283589376752 vs singleton = 1.0 vs classical pivot = 1.2438998081093664\n",
      "agreement = 0.9322292350416561 vs dynamic pivot = 1.0044837162332745 vs singleton = 1.0 vs classical pivot = 1.247760666498359\n"
     ]
    }
   ],
   "source": [
    "for csv_file_path in datasets:\n",
    "    # Create lists to store objective values for each dataset\n",
    "    graph_adjacency_lists = create_graph_from_csv(csv_file_path)\n",
    "    # Generate a random node arrival order\n",
    "    random_node_order = generate_random_node_order(graph_adjacency_lists)\n",
    "\n",
    "    # Initialization for the random variables of dynamic pivot\n",
    "    # we first contract a random permulation pi: node --> order in the random permulation\n",
    "    pi = {element: index for index, element in enumerate(generate_random_node_order(graph_adjacency_lists))}\n",
    "    eta = {key: key for key in pi.keys()}\n",
    "    pivot_clustering = {key: key for key in pi.keys()}\n",
    "\n",
    "    # Initializaiton for the random variables of agreement\n",
    "    sparse_graph = {}\n",
    "    nodes_present_at_the_moment = OptList()\n",
    "    Phi = set()\n",
    "    Phi_nodes = {}\n",
    "    I_nodes = defaultdict(set)\n",
    "    B_nodes = {node:OptList() for node in graph_adjacency_lists}\n",
    "\n",
    "\n",
    "    # Initialize an empty dictionary to store the current graph\n",
    "    current_graph = {}\n",
    "    # p is the probabilty of a random deletion at each time step\n",
    "    random_node_iterator = iter(random_node_order)\n",
    "    i = -1\n",
    "    only_deletion = False\n",
    "    total_iterations = 2* len(graph_adjacency_lists)\n",
    "    #### Here starts the copy paste\n",
    "\n",
    "    while True:\n",
    "        i +=1\n",
    "        if (i > 100) and (not current_graph):\n",
    "            print(\"Oh, now the graph is empty\")\n",
    "            break\n",
    "        deletion = False\n",
    "        try:\n",
    "            # node addition\n",
    "            node = next(random_node_iterator)\n",
    "        except StopIteration:\n",
    "            # StopIteration is raised when the iterator is exhausted\n",
    "            clustering = connected_components(sparse_graph)\n",
    "            all_singletons = {element: index for index, element in enumerate(sparse_graph.keys())}\n",
    "            pivot_clustering_this_step = {u: pivot_clustering[u] for u in sparse_graph.keys()}\n",
    "            classical_pivot_clustering_sol = classical_pivot(graph_adjacency_lists)\n",
    "            \n",
    "            # Calculate correlation values and store them\n",
    "            corr_clustering = correlation_clustering_value(current_graph, clustering)\n",
    "            corr_all_singletons = correlation_clustering_value(current_graph, all_singletons)\n",
    "            corr_pivot_clustering = correlation_clustering_value(current_graph, pivot_clustering_this_step)\n",
    "            classical_pivot_clustering = correlation_clustering_value(current_graph, classical_pivot_clustering_sol)\n",
    "            \n",
    "\n",
    "            correlation_values_clustering = corr_clustering/corr_all_singletons\n",
    "            correlation_values_all_singletons = corr_all_singletons/corr_all_singletons\n",
    "            correlation_values_pivot_clustering = corr_pivot_clustering/corr_all_singletons\n",
    "            correlation_values_classical_pivot_clustering = classical_pivot_clustering/corr_all_singletons\n",
    "            break        \n",
    "        current_graph[node] = OptList()\n",
    "        nodes_present_at_the_moment.append(node)\n",
    "        # Add edges to previously arrived nodes\n",
    "        for neighbor in graph_adjacency_lists[node]:\n",
    "            if neighbor in nodes_present_at_the_moment:\n",
    "                # I have to update the pivot clustering\n",
    "                # this is the case where node may become the new pivot of neighbor\n",
    "                if pi[eta[neighbor]] > pi[node]:\n",
    "                    if eta[neighbor] == neighbor:\n",
    "                        for w in current_graph[neighbor]:\n",
    "                            if w == neighbor:\n",
    "                                continue\n",
    "                            pivot_clustering[w] = w if eta[w]==neighbor else pivot_clustering[w]\n",
    "                    eta[neighbor] = node\n",
    "                    pivot_clustering[neighbor] = node if eta[node]==node else neighbor\n",
    "\n",
    "                if pi[eta[node]] > pi[neighbor]:\n",
    "                    if eta[node] == node:\n",
    "                        for w in current_graph[node]:\n",
    "                            if w == node:\n",
    "                                continue\n",
    "                            pivot_clustering[w] = w if eta[w]==node else pivot_clustering[w]\n",
    "                    eta[node] = neighbor\n",
    "                    pivot_clustering[node] = neighbor if eta[neighbor]==neighbor else node\n",
    "                current_graph[neighbor].append(node)\n",
    "                current_graph[node].append(neighbor)\n",
    "        # Notify\n",
    "        # ---------send Type 0 notifications-------------------\n",
    "        notified_so_far = set()\n",
    "        notified_so_far.add(node)\n",
    "        got_type_0_notification = current_graph[node].getRandom(notifications)\n",
    "        if got_type_0_notification:\n",
    "            got_type_0_notification.discard(node)\n",
    "            I_nodes[node].update(got_type_0_notification)\n",
    "            for v in got_type_0_notification:\n",
    "                B_nodes[v].append(node)\n",
    "\n",
    "        # ---------send Type 1 notifications-------------------\n",
    "        got_type_1_notification = set()\n",
    "        for v in got_type_0_notification:\n",
    "            if v not in current_graph:\n",
    "                continue\n",
    "            # Avoid that the notification graph becomes too dense\n",
    "            for u in I_nodes[v]:\n",
    "                B_nodes[u].remove(v)\n",
    "            # take another sample 1\n",
    "            br = current_graph[v].getRandom(notifications)\n",
    "            I_nodes[v].update(current_graph[v].getRandom(notifications))\n",
    "            I_nodes[v].discard(v)\n",
    "            I_nodes[v].discard(node)\n",
    "            for u in I_nodes[v]:\n",
    "                B_nodes[u].append(v)\n",
    "                if u not in notified_so_far:\n",
    "                    got_type_1_notification.add(u)\n",
    "                    notified_so_far.add(u)\n",
    "        # ---------send Type 2 notifications-------------------\n",
    "        got_type_2_notification = set()\n",
    "        for v in got_type_1_notification:\n",
    "            if v not in current_graph:\n",
    "                continue\n",
    "            for u in I_nodes[v]:\n",
    "                B_nodes[u].remove(v)\n",
    "            # take another sample 2\n",
    "            br = current_graph[v].getRandom(notifications)\n",
    "            I_nodes[v].update(current_graph[v].getRandom(notifications))\n",
    "            I_nodes[v].discard(v)\n",
    "            I_nodes[v].discard(node)\n",
    "            for u in I_nodes[v]:\n",
    "                B_nodes[u].append(v)\n",
    "                if (u not in notified_so_far) or (u not in got_type_2_notification) :\n",
    "                    got_type_2_notification.add(u)\n",
    "        # ---------receive Type 2 notifications-------------------\n",
    "        for v in got_type_2_notification:\n",
    "            if v not in current_graph:\n",
    "                continue\n",
    "            # Update the sample\n",
    "            for u in I_nodes[v]:\n",
    "                B_nodes[u].remove(v)\n",
    "            I_nodes[v] = current_graph[v].getRandom(notifications)\n",
    "            I_nodes[v].discard(v)\n",
    "            I_nodes[v].discard(node)\n",
    "            for u in I_nodes[v]:\n",
    "                B_nodes[u].append(v)\n",
    "        # First we check if it is a deletion or addition and initialize the respective variables\n",
    "        sparse_graph[node] = OptList()\n",
    "        Phi_nodes[node] = set()\n",
    "        # The interesting event set corresponds to the notified_so_far\n",
    "        for u in notified_so_far:\n",
    "            # If u is in Phi we first delete all its edges to nodes not in Phi\n",
    "            if u in Phi:\n",
    "                neighbors_of_u = [v for v in sparse_graph[u]]\n",
    "                for v in neighbors_of_u:\n",
    "                    if (u == v) or (v in Phi):\n",
    "                        continue\n",
    "                    sparse_graph[u].remove(v)\n",
    "                    sparse_graph[v].remove(u)\n",
    "                    Phi_nodes[v].discard(u)\n",
    "            Phi.discard(u)\n",
    "            # Anchor procedure\n",
    "            anchor_prob = anchor_probability_numerator / len(current_graph[u])\n",
    "            if (random.random() < anchor_prob) and ProbAHeaviness(u, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                Phi.add(u)\n",
    "                for v in current_graph[u]:\n",
    "                    if ProbAgreement(u, v, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                        sparse_graph[u].append(v)\n",
    "                        sparse_graph[v].append(u)\n",
    "                        if u != v:\n",
    "                            Phi_nodes[v].add(u)\n",
    "            # Clean procedure\n",
    "            connected_nodes_in_anchor_set = [w for w in Phi_nodes[u]]\n",
    "            for w in connected_nodes_in_anchor_set:\n",
    "                if w not in nodes_present_at_the_moment:\n",
    "                    Phi_nodes[u].discard(w)\n",
    "                    continue\n",
    "                if w == u:\n",
    "                    continue\n",
    "                if ProbAgreement(u, w, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation) and ProbAHeaviness(w, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                    continue\n",
    "                else:\n",
    "                    sparse_graph[u].remove(w)\n",
    "                    sparse_graph[w].remove(u)\n",
    "                    Phi_nodes[u].discard(w)\n",
    "            # Connect procedure\n",
    "            J_u = current_graph[u].getRandom(num_samples_for_connect_procedure)\n",
    "            for w in J_u:\n",
    "                phi_nodes_w = [r for r in Phi_nodes[w]]\n",
    "                for r in phi_nodes_w:\n",
    "                    if r not in nodes_present_at_the_moment:\n",
    "                        Phi_nodes[w].discard(r)\n",
    "                        continue\n",
    "                    if r not in current_graph[u]:\n",
    "                        continue\n",
    "                    if ProbAgreement(u, r, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation) and ProbAHeaviness(r, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                        sparse_graph[u].append(r)\n",
    "                        sparse_graph[r].append(u)\n",
    "                        Phi_nodes[u].add(r)\n",
    "\n",
    "        # Plot the results\n",
    "        \n",
    "    \n",
    "    print(f\"agreement = {correlation_values_clustering} vs dynamic pivot = {correlation_values_pivot_clustering} vs singleton = {correlation_values_all_singletons} vs classical pivot = {correlation_values_classical_pivot_clustering}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
