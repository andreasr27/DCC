{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bec225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91cb6f",
   "metadata": {},
   "source": [
    "# Load the datapoints into X. The i-th line of matrix X corresponds to the embedding vector of node i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c46b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "file_path = 'datasets/all_data.txt'  # Replace with the actual file path\n",
    "\n",
    "# Initialize an empty list to store the matrices\n",
    "matrices = []\n",
    "\n",
    "# Read the file line by line\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Extract numerical values and create the embedding\n",
    "        embedding = [float(element.split(\":\")[1]) for element in \" \".join(line.split(\" \")[1:]).split()]\n",
    "        \n",
    "        # Append the embedding to the matrices list\n",
    "        matrices.append(embedding)\n",
    "X = np.array(matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0253e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = np.sum(X**2, axis=1, keepdims=True)\n",
    "sy = np.sum(X**2, axis=1, keepdims=True)\n",
    "distances = -2 * X.dot(X.T) + sx + sy.T\n",
    "distances[distances < 1] = 0\n",
    "distances = np.sqrt(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a1e68",
   "metadata": {},
   "source": [
    "### We analyze the impact of the density of the graph to running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b8a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_probability_numerator = 20.\n",
    "notifications = 2\n",
    "epsilon = 0.2\n",
    "deletion_prob_at_each_step = 0.2\n",
    "num_samples_for_agreement_and_heavyness_calculation = 2\n",
    "num_samples_for_connect_procedure = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66d2a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes = 13910, number of edges 3524178==> density = 253.35571531272467\n",
      " agreement = 0.6879221860488065 vs pivot = 0.5846112675675879 vs singleton = 1.0\n",
      " agreement = 0.6879221860488065 vs pivot = 0.5846112675675879 vs singleton = 1.0\n",
      "agreement time = 12.047162055969238 vs pivot total time = 0.2792983055114746\n",
      "agreement worst update time = 0.058423757553100586 vs pivot worst update time = 0.00011515617370605469\n",
      "agreement time for additions = 7.640567779541016\n",
      "agreement worst update time for additions = 0.05842471122741699\n",
      "agreement time for deletions = 4.411747455596924\n",
      "agreement worst update time for deletions = 0.015746116638183594\n",
      "=========================================================================================================\n",
      "number of nodes = 13910, number of edges 1597818==> density = 114.86829618979152\n",
      " agreement = 0.6276267248219037 vs pivot = 0.5071627653963694 vs singleton = 1.0\n",
      " agreement = 0.6276267248219037 vs pivot = 0.5071627653963694 vs singleton = 1.0\n",
      "agreement time = 14.810634136199951 vs pivot total time = 0.1801755428314209\n",
      "agreement worst update time = 0.05066108703613281 vs pivot worst update time = 0.0001590251922607422\n",
      "agreement time for additions = 9.545376539230347\n",
      "agreement worst update time for additions = 0.05066180229187012\n",
      "agreement time for deletions = 5.2704620361328125\n",
      "agreement worst update time for deletions = 0.02401900291442871\n",
      "=========================================================================================================\n",
      "number of nodes = 13910, number of edges 970078==> density = 69.73961179007908\n",
      " agreement = 0.5359185855552433 vs pivot = 0.5194793645711676 vs singleton = 1.0\n",
      " agreement = 0.5359185855552433 vs pivot = 0.5194793645711676 vs singleton = 1.0\n",
      "agreement time = 16.00560688972473 vs pivot total time = 0.11380863189697266\n",
      "agreement worst update time = 0.046417236328125 vs pivot worst update time = 5.984306335449219e-05\n",
      "agreement time for additions = 10.038501024246216\n",
      "agreement worst update time for additions = 0.046417236328125\n",
      "agreement time for deletions = 5.972062587738037\n",
      "agreement worst update time for deletions = 0.01852583885192871\n",
      "=========================================================================================================\n",
      "number of nodes = 13910, number of edges 725672==> density = 52.16908698777858\n",
      " agreement = 0.37863539056217893 vs pivot = 0.4342817611566179 vs singleton = 1.0\n",
      " agreement = 0.37863539056217893 vs pivot = 0.4342817611566179 vs singleton = 1.0\n",
      "agreement time = 13.940442562103271 vs pivot total time = 0.09900212287902832\n",
      "agreement worst update time = 0.04682302474975586 vs pivot worst update time = 5.1975250244140625e-05\n",
      "agreement time for additions = 9.03708815574646\n",
      "agreement worst update time for additions = 0.046823978424072266\n",
      "agreement time for deletions = 4.908176422119141\n",
      "agreement worst update time for deletions = 0.015321969985961914\n",
      "=========================================================================================================\n",
      "number of nodes = 13910, number of edges 587676==> density = 42.24845434938893\n",
      " agreement = 0.3019463763952303 vs pivot = 0.3447436482235195 vs singleton = 1.0\n",
      " agreement = 0.3019463763952303 vs pivot = 0.3447436482235195 vs singleton = 1.0\n",
      "agreement time = 13.421088218688965 vs pivot total time = 0.08300971984863281\n",
      "agreement worst update time = 0.05628180503845215 vs pivot worst update time = 3.409385681152344e-05\n",
      "agreement time for additions = 8.661384344100952\n",
      "agreement worst update time for additions = 0.056282758712768555\n",
      "agreement time for deletions = 4.76443886756897\n",
      "agreement worst update time for deletions = 0.017090797424316406\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "thresholds = [np.mean(distances)/10.0, np.mean(distances)/15.0, np.mean(distances)/20.0, np.mean(distances)/25.0, np.mean(distances)/30.0]\n",
    "\n",
    "\n",
    "n, m = distances.shape\n",
    "\n",
    "num_experiments = 1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    adjacency_lists = {}\n",
    "    # self loops are added because diagonal elements have a value of 1\n",
    "    for a in range(0,n):\n",
    "        for b in range(0,m):\n",
    "            if distances[a,b] > threshold:\n",
    "                continue\n",
    "            # Add edge to the adjacency lists\n",
    "            if a not in adjacency_lists:\n",
    "                adjacency_lists[a] = OptList()\n",
    "            adjacency_lists[a].append(b)\n",
    "\n",
    "            if b not in adjacency_lists:\n",
    "                adjacency_lists[b] = OptList()\n",
    "            adjacency_lists[b].append(a)\n",
    "            \n",
    "    # calculate the objective of the classical pivot on the entire graph\n",
    "    classical_pivot_clustering_sol = classical_pivot(graph_adjacency_lists)\n",
    "    classical_pivot_clustering = correlation_clustering_value(current_graph, classical_pivot_clustering_sol)\n",
    "\n",
    "    \n",
    "    # now we have created a graph\n",
    "\n",
    "    # These are the statistics of nodes and edges\n",
    "    nodes = len(adjacency_lists.keys())\n",
    "    edges = sum([len(adjacency_lists[node]) for node in adjacency_lists.keys()])\n",
    "    density = edges/nodes\n",
    "\n",
    "    print(f\"number of nodes = {nodes}, number of edges {edges}==> density = {density}\")\n",
    "\n",
    "    graph_adjacency_lists = adjacency_lists\n",
    "    \n",
    "    # this it to measure the time in each execution\n",
    "    dynamic_pivot_total_time = .0\n",
    "    agreement_total_time = .0\n",
    "    dynamic_pivot_worst_update_time = .0\n",
    "    agreement_worst_update_time = .0\n",
    "\n",
    "    agreement_total_time_node_additions = .0\n",
    "    agreement_total_time_node_deletions = .0\n",
    "\n",
    "    agreement_worst_update_time_node_additions = .0\n",
    "    agreement_worst_update_time_node_deletions = .0\n",
    "    \n",
    "     # Create lists to store objective values for each dataset\n",
    "    correlation_values_clustering = []\n",
    "    correlation_values_all_singletons = []\n",
    "    correlation_values_pivot_clustering = []\n",
    "\n",
    "\n",
    "    for t in range(0, num_experiments):\n",
    "        random.seed(t)\n",
    "\n",
    "        # Generate a random node arrival order\n",
    "        random_node_order = generate_random_node_order(graph_adjacency_lists)\n",
    "\n",
    "        # Initialization for the random variables of dynamic pivot\n",
    "        # we first contract a random permulation pi: node --> order in the random permulation\n",
    "        pi = {element: index for index, element in enumerate(generate_random_node_order(graph_adjacency_lists))}\n",
    "        eta = {key: key for key in pi.keys()}\n",
    "        pivot_clustering = {key: key for key in pi.keys()}\n",
    "\n",
    "        #Initializaiton for the random variables of agreement\n",
    "        sparse_graph = {}\n",
    "        nodes_present_at_the_moment = OptList()\n",
    "        Phi = set()\n",
    "        Phi_nodes = {}\n",
    "        I_nodes = defaultdict(set)\n",
    "        B_nodes = {node:OptList() for node in graph_adjacency_lists}\n",
    "        # Initialize an empty dictionary to store the current graph\n",
    "        current_graph = {}\n",
    "        random_node_iterator = iter(random_node_order)\n",
    "        i = -1\n",
    "        only_deletion = False\n",
    "        cleaning = 0\n",
    "        while True:\n",
    "            i +=1\n",
    "            if (i > 100) and (not current_graph):\n",
    "                break\n",
    "            if ((random.random() < deletion_prob_at_each_step) and (i > 0) and current_graph) or (only_deletion and current_graph):\n",
    "                deletion = True\n",
    "                # node deletion\n",
    "                node = nodes_present_at_the_moment.getRandom()\n",
    "                neighbors_to_delete = [neighbor for neighbor in current_graph[node]]\n",
    "                start_time = time.time()\n",
    "                for neighbor in neighbors_to_delete:\n",
    "                    if not current_graph[neighbor].remove(node):\n",
    "                        print(\"We have a problem Houston\")\n",
    "                    if node == neighbor:\n",
    "                        continue\n",
    "                    # start timer for pivot\n",
    "                    # we now update the pivot clustering\n",
    "                    if (eta[neighbor] != node) or (eta[node] != neighbor):\n",
    "                        # no need for update in this case\n",
    "                        continue\n",
    "                    # when node may have been the pivot of neighbor\n",
    "                    if eta[neighbor] == node:\n",
    "                        eta[neighbor] = neighbor\n",
    "                        for neighbor_of_neighbor in current_graph[neighbor]:\n",
    "                            if pi[neighbor_of_neighbor] < pi[eta[neighbor]]:\n",
    "                                eta[neighbor] = neighbor_of_neighbor\n",
    "                        if eta[neighbor] == neighbor:\n",
    "                            pivot_clustering[neighbor] = neighbor\n",
    "                            for neighbor_of_neighbor in current_graph[neighbor]:\n",
    "                                if neighbor_of_neighbor == neighbor:\n",
    "                                    continue\n",
    "                                pivot_clustering[neighbor_of_neighbor] = neighbor if eta[neighbor_of_neighbor] == neighbor else pivot_clustering[neighbor_of_neighbor]\n",
    "                        else:\n",
    "                            pivot_clustering[neighbor] = eta[neighbor] if eta[neighbor] == eta[eta[neighbor]] else neighbor\n",
    "                    # when eta[node] == neighbor. This neighbor may have been the pivot of node\n",
    "                    else:\n",
    "                        eta[node] = node\n",
    "                        for neighbor_of_neighbor in current_graph[node]:\n",
    "                            if pi[neighbor_of_neighbor] < pi[eta[node]]:\n",
    "                                eta[node] = neighbor_of_neighbor\n",
    "                        if eta[node] == node:\n",
    "                            pivot_clustering[node] = node\n",
    "                            for neighbor_of_neighbor in current_graph[node]:\n",
    "                                if neighbor_of_neighbor == node:\n",
    "                                    continue\n",
    "                                pivot_clustering[neighbor_of_neighbor] = node if eta[neighbor_of_neighbor] == node else pivot_clustering[neighbor_of_neighbor]\n",
    "                        else:\n",
    "                            pivot_clustering[node] = eta[node] if eta[node] == eta[eta[node]] else node\n",
    "                    dynamic_pivot_total_time += (time.time() - start_time)\n",
    "                    dynamic_pivot_worst_update_time = max(dynamic_pivot_worst_update_time, time.time() - start_time)\n",
    "\n",
    "                nodes_present_at_the_moment.remove(node)\n",
    "\n",
    "                del current_graph[node]\n",
    "            else:\n",
    "                deletion = False\n",
    "                try:\n",
    "                    # node addition\n",
    "                    node = next(random_node_iterator)\n",
    "                except StopIteration:\n",
    "                    # StopIteration is raised when the iterator is exhausted\n",
    "                    only_deletion = True\n",
    "                    continue\n",
    "                current_graph[node] = OptList()\n",
    "                nodes_present_at_the_moment.append(node)\n",
    "                # Add edges to previously arrived nodes\n",
    "                for neighbor in graph_adjacency_lists[node]:\n",
    "                    if neighbor in nodes_present_at_the_moment:\n",
    "                        # I have to update the pivot clustering\n",
    "                        start_time = time.time()\n",
    "                        # this is the case where node may become the new pivot of neighbor\n",
    "                        if pi[eta[neighbor]] > pi[node]:\n",
    "                            if eta[neighbor] == neighbor:\n",
    "                                for w in current_graph[neighbor]:\n",
    "                                    if w == neighbor:\n",
    "                                        continue\n",
    "                                    pivot_clustering[w] = w if eta[w]==neighbor else pivot_clustering[w]\n",
    "                            eta[neighbor] = node\n",
    "                            pivot_clustering[neighbor] = node if eta[node]==node else neighbor\n",
    "\n",
    "                        if pi[eta[node]] > pi[neighbor]:\n",
    "                            if eta[node] == node:\n",
    "                                for w in current_graph[node]:\n",
    "                                    if w == node:\n",
    "                                        continue\n",
    "                                    pivot_clustering[w] = w if eta[w]==node else pivot_clustering[w]\n",
    "                            eta[node] = neighbor\n",
    "                            pivot_clustering[node] = neighbor if eta[neighbor]==neighbor else node\n",
    "                        dynamic_pivot_total_time += (time.time() - start_time)\n",
    "                        dynamic_pivot_worst_update_time = max(dynamic_pivot_worst_update_time, time.time() - start_time)\n",
    "                        current_graph[neighbor].append(node)\n",
    "                        current_graph[node].append(neighbor)\n",
    "\n",
    "                # --------------------------------------Notify--------------------------------------\n",
    "                # ---------send Type 0 notifications-------------------\n",
    "            start_time = time.time()\n",
    "            notified_so_far = set()\n",
    "            if deletion:\n",
    "                for v in I_nodes[node]:\n",
    "                    if v == node:\n",
    "                        print(\"We may have a problem Houston\")\n",
    "                    B_nodes[v].remove(node)\n",
    "                got_type_0_notification = B_nodes[node].getRandom(notifications)\n",
    "                # we need to remove node, from any forward notification set\n",
    "                if not got_type_0_notification:\n",
    "                    continue\n",
    "                got_type_0_notification.discard(node)\n",
    "                if not got_type_0_notification:\n",
    "                    continue\n",
    "            else:\n",
    "                notified_so_far.add(node)\n",
    "                got_type_0_notification = current_graph[node].getRandom(notifications)\n",
    "                if got_type_0_notification:\n",
    "                    got_type_0_notification.discard(node)\n",
    "                    I_nodes[node].update(got_type_0_notification)\n",
    "                    for v in got_type_0_notification:\n",
    "                        B_nodes[v].append(node)\n",
    "\n",
    "            # ---------send Type 1 notifications-------------------\n",
    "            got_type_1_notification = set()\n",
    "            for v in got_type_0_notification:\n",
    "                if v not in current_graph:\n",
    "                    continue\n",
    "                # Avoid that the notification graph becomes too dense\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].remove(v)\n",
    "                # take another sample 1\n",
    "                br = current_graph[v].getRandom(notifications)\n",
    "                I_nodes[v].update(current_graph[v].getRandom(notifications))\n",
    "                I_nodes[v].discard(v)\n",
    "                I_nodes[v].discard(node)\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].append(v)\n",
    "                    if u not in notified_so_far:\n",
    "                        got_type_1_notification.add(u)\n",
    "                        notified_so_far.add(u)\n",
    "\n",
    "            # ---------send Type 2 notifications-------------------\n",
    "            got_type_2_notification = set()\n",
    "            for v in got_type_1_notification:\n",
    "                if v not in current_graph:\n",
    "                    continue\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].remove(v)\n",
    "                # take another sample 2\n",
    "                br = current_graph[v].getRandom(notifications)\n",
    "                I_nodes[v].update(current_graph[v].getRandom(notifications))\n",
    "                I_nodes[v].discard(v)\n",
    "                I_nodes[v].discard(node)\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].append(v)\n",
    "                    if (u not in notified_so_far) or (u not in got_type_2_notification) :\n",
    "                        got_type_2_notification.add(u)\n",
    "\n",
    "            # ---------receive Type 2 notifications-------------------\n",
    "            for v in got_type_2_notification:\n",
    "                if v not in current_graph:\n",
    "                    continue\n",
    "                # just update the sample\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].remove(v)\n",
    "                I_nodes[v] = current_graph[v].getRandom(notifications)\n",
    "                I_nodes[v].discard(v)\n",
    "                I_nodes[v].discard(node)\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].append(v)\n",
    "\n",
    "            # We check if it is a deletion or addition and initialize the respective\n",
    "            # variables\n",
    "            if not deletion:\n",
    "                sparse_graph[node] = OptList()\n",
    "                Phi_nodes[node] = set()\n",
    "            else:\n",
    "                Phi.discard(node)\n",
    "                neighbors_to_delete = [neighbor for neighbor in sparse_graph[node]]\n",
    "                for neighbor in neighbors_to_delete:\n",
    "                    sparse_graph[neighbor].remove(node)\n",
    "                    sparse_graph[node].remove(neighbor)\n",
    "                    Phi_nodes[neighbor].discard(node)\n",
    "                del sparse_graph[node]\n",
    "                del Phi_nodes[node]\n",
    "\n",
    "            # The interesting event set corresponds to the notified_so_far\n",
    "            for u in notified_so_far:\n",
    "                if deletion and node == u:\n",
    "                    continue\n",
    "                if u not in nodes_present_at_the_moment:\n",
    "                    continue\n",
    "                    print(\"Another problem\")\n",
    "                # if u is in Phi we first delete all its edges to nodes not in Phi\n",
    "                if u in Phi:\n",
    "                    neighbors_of_u = [v for v in sparse_graph[u]]\n",
    "                    for v in neighbors_of_u:\n",
    "                        if (u == v) or (v in Phi):\n",
    "                            continue\n",
    "                        sparse_graph[u].remove(v)\n",
    "                        sparse_graph[v].remove(u)\n",
    "                        Phi_nodes[v].discard(u)\n",
    "                Phi.discard(u)\n",
    "                # we now implement the Anchor procedure\n",
    "                anchor_prob = anchor_probability_numerator / len(current_graph[u])\n",
    "                if (random.random() < anchor_prob) and ProbAHeaviness(u, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                    Phi.add(u)\n",
    "                    for v in current_graph[u]:\n",
    "                        if ProbAgreement(u, v, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                            sparse_graph[u].append(v)\n",
    "                            sparse_graph[v].append(u)\n",
    "                            if u != v:\n",
    "                                Phi_nodes[v].add(u)\n",
    "\n",
    "                # Now we implement the Clean procedure\n",
    "                connected_nodes_in_anchor_set = [w for w in Phi_nodes[u]]\n",
    "                for w in connected_nodes_in_anchor_set:\n",
    "                    if w not in nodes_present_at_the_moment:\n",
    "                        Phi_nodes[u].discard(w)\n",
    "                        #del sparse_graph[w]\n",
    "                        continue\n",
    "                    if w == u:\n",
    "                        continue\n",
    "                    if ProbAgreement(u, w, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation) and ProbAHeaviness(w, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                        continue\n",
    "                    else:\n",
    "                        cleaning+=1\n",
    "                        sparse_graph[u].remove(w)\n",
    "                        sparse_graph[w].remove(u)\n",
    "                        Phi_nodes[u].discard(w)\n",
    "\n",
    "                # Now we implement the Connect procedure\n",
    "                J_u = current_graph[u].getRandom(num_samples_for_connect_procedure)\n",
    "                for w in J_u:\n",
    "                    phi_nodes_w = [r for r in Phi_nodes[w]]\n",
    "                    for r in phi_nodes_w:\n",
    "                        if r not in nodes_present_at_the_moment:\n",
    "                            Phi_nodes[w].discard(r)\n",
    "                            continue\n",
    "                        if r not in current_graph[u]:\n",
    "                            continue\n",
    "                        if ProbAgreement(u, r, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation) and ProbAHeaviness(r, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                            sparse_graph[u].append(r)\n",
    "                            sparse_graph[r].append(u)\n",
    "                            Phi_nodes[u].add(r)\n",
    "            agreement_total_time+=(time.time() - start_time)\n",
    "            agreement_worst_update_time = max(agreement_worst_update_time, time.time() - start_time)\n",
    "            if deletion:\n",
    "                agreement_total_time_node_deletions+=(time.time() - start_time)\n",
    "                agreement_worst_update_time_node_deletions = max(agreement_worst_update_time_node_deletions, time.time() - start_time)\n",
    "            else:\n",
    "                agreement_total_time_node_additions+=(time.time() - start_time)\n",
    "                agreement_worst_update_time_node_additions = max(agreement_worst_update_time_node_additions, time.time() - start_time)\n",
    "            if agreement_worst_update_time > 20.5:\n",
    "                print(f\"Deletion = {deletion}\")\n",
    "            if (i+1) % 500 == 0:\n",
    "                clustering = connected_components(sparse_graph)\n",
    "                all_singletons = {element: index for index, element in enumerate(sparse_graph.keys())}\n",
    "                pivot_clustering_this_step = {u: pivot_clustering[u] for u in sparse_graph.keys()}\n",
    "\n",
    "                # Calculate correlation values and store them\n",
    "                corr_clustering = correlation_clustering_value(current_graph, clustering)\n",
    "                corr_all_singletons = correlation_clustering_value(current_graph, all_singletons)\n",
    "                corr_pivot_clustering = correlation_clustering_value(current_graph, pivot_clustering_this_step)\n",
    "                if corr_all_singletons != 0:\n",
    "                    correlation_values_clustering.append(corr_clustering/corr_all_singletons)\n",
    "                    correlation_values_all_singletons.append(corr_all_singletons/corr_all_singletons)\n",
    "                    correlation_values_pivot_clustering.append(corr_pivot_clustering/corr_all_singletons)\n",
    "    print(f\" agreement = {np.mean(correlation_values_clustering)} vs pivot = \"\n",
    "      f\"{np.mean(correlation_values_pivot_clustering)} vs singleton = {np.mean(correlation_values_all_singletons)}\")\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    print(f\" agreement = {np.mean(correlation_values_clustering)} vs pivot = \"\n",
    "      f\"{np.mean(correlation_values_pivot_clustering)} vs singleton = {np.mean(correlation_values_all_singletons)}\")                        \n",
    "    print(f\"agreement time = {agreement_total_time/float(num_experiments)} vs pivot total time = {dynamic_pivot_total_time/float(num_experiments)}\")\n",
    "    print(f\"agreement worst update time = {agreement_worst_update_time} vs pivot worst update time = {dynamic_pivot_worst_update_time}\")\n",
    "\n",
    "    print(f\"agreement time for additions = {agreement_total_time_node_additions/float(num_experiments)}\")\n",
    "    print(f\"agreement worst update time for additions = {agreement_worst_update_time_node_additions}\")\n",
    "\n",
    "    print(f\"agreement time for deletions = {agreement_total_time_node_deletions/float(num_experiments)}\")\n",
    "    print(f\"agreement worst update time for deletions = {agreement_worst_update_time_node_deletions}\")\n",
    "    \n",
    "    print(\"=========================================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17311f0",
   "metadata": {},
   "source": [
    "We compare the performace of classical pivot to the solution of the dynamic algorithms on the entire graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [np.mean(distances)/10.0, np.mean(distances)/15.0, np.mean(distances)/20.0, np.mean(distances)/25.0, np.mean(distances)/30.0]\n",
    "\n",
    "\n",
    "n, m = distances.shape\n",
    "\n",
    "num_experiments = 1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    adjacency_lists = {}\n",
    "    # self loops are added because diagonal elements have a value of 1\n",
    "    for a in range(0,n):\n",
    "        for b in range(0,m):\n",
    "            if distances[a,b] > threshold:\n",
    "                continue\n",
    "            # Add edge to the adjacency lists\n",
    "            if a not in adjacency_lists:\n",
    "                adjacency_lists[a] = OptList()\n",
    "            adjacency_lists[a].append(b)\n",
    "\n",
    "            if b not in adjacency_lists:\n",
    "                adjacency_lists[b] = OptList()\n",
    "            adjacency_lists[b].append(a)\n",
    "            \n",
    "    # calculate the objective of the classical pivot on the entire graph\n",
    "    classical_pivot_clustering_sol = classical_pivot(graph_adjacency_lists)\n",
    "    classical_pivot_clustering = correlation_clustering_value(current_graph, classical_pivot_clustering_sol)\n",
    "\n",
    "    \n",
    "    # now we have created a graph\n",
    "\n",
    "    # These are the statistics of nodes and edges\n",
    "    nodes = len(adjacency_lists.keys())\n",
    "    edges = sum([len(adjacency_lists[node]) for node in adjacency_lists.keys()])\n",
    "    density = edges/nodes\n",
    "\n",
    "    print(f\"number of nodes = {nodes}, number of edges {edges}==> density = {density}\")\n",
    "\n",
    "    graph_adjacency_lists = adjacency_lists\n",
    "    \n",
    "    # this it to measure the time in each execution\n",
    "    dynamic_pivot_total_time = .0\n",
    "    agreement_total_time = .0\n",
    "    dynamic_pivot_worst_update_time = .0\n",
    "    agreement_worst_update_time = .0\n",
    "\n",
    "    agreement_total_time_node_additions = .0\n",
    "    agreement_total_time_node_deletions = .0\n",
    "\n",
    "    agreement_worst_update_time_node_additions = .0\n",
    "    agreement_worst_update_time_node_deletions = .0\n",
    "    \n",
    "     # Create lists to store objective values for each dataset\n",
    "    correlation_values_clustering = []\n",
    "    correlation_values_all_singletons = []\n",
    "    correlation_values_pivot_clustering = []\n",
    "\n",
    "\n",
    "    for t in range(0, num_experiments):\n",
    "        random.seed(t)\n",
    "\n",
    "        # Generate a random node arrival order\n",
    "        random_node_order = generate_random_node_order(graph_adjacency_lists)\n",
    "\n",
    "        # Initialization for the random variables of dynamic pivot\n",
    "        # we first contract a random permulation pi: node --> order in the random permulation\n",
    "        pi = {element: index for index, element in enumerate(generate_random_node_order(graph_adjacency_lists))}\n",
    "        eta = {key: key for key in pi.keys()}\n",
    "        pivot_clustering = {key: key for key in pi.keys()}\n",
    "\n",
    "        #Initializaiton for the random variables of agreement\n",
    "        sparse_graph = {}\n",
    "        nodes_present_at_the_moment = OptList()\n",
    "        Phi = set()\n",
    "        Phi_nodes = {}\n",
    "        I_nodes = defaultdict(set)\n",
    "        B_nodes = {node:OptList() for node in graph_adjacency_lists}\n",
    "        # Initialize an empty dictionary to store the current graph\n",
    "        current_graph = {}\n",
    "        random_node_iterator = iter(random_node_order)\n",
    "        i = -1\n",
    "        only_deletion = False\n",
    "        cleaning = 0\n",
    "        while True:\n",
    "            i +=1\n",
    "            if (i > 100) and (not current_graph):\n",
    "                break\n",
    "            if ((random.random() < 1) and (i > 0) and current_graph) or (only_deletion and current_graph):\n",
    "                deletion = True\n",
    "                # node deletion\n",
    "                node = nodes_present_at_the_moment.getRandom()\n",
    "                neighbors_to_delete = [neighbor for neighbor in current_graph[node]]\n",
    "                start_time = time.time()\n",
    "                for neighbor in neighbors_to_delete:\n",
    "                    if not current_graph[neighbor].remove(node):\n",
    "                        print(\"We have a problem Houston\")\n",
    "                    if node == neighbor:\n",
    "                        continue\n",
    "                    # start timer for pivot\n",
    "                    # we now update the pivot clustering\n",
    "                    if (eta[neighbor] != node) or (eta[node] != neighbor):\n",
    "                        # no need for update in this case\n",
    "                        continue\n",
    "                    # when node may have been the pivot of neighbor\n",
    "                    if eta[neighbor] == node:\n",
    "                        eta[neighbor] = neighbor\n",
    "                        for neighbor_of_neighbor in current_graph[neighbor]:\n",
    "                            if pi[neighbor_of_neighbor] < pi[eta[neighbor]]:\n",
    "                                eta[neighbor] = neighbor_of_neighbor\n",
    "                        if eta[neighbor] == neighbor:\n",
    "                            pivot_clustering[neighbor] = neighbor\n",
    "                            for neighbor_of_neighbor in current_graph[neighbor]:\n",
    "                                if neighbor_of_neighbor == neighbor:\n",
    "                                    continue\n",
    "                                pivot_clustering[neighbor_of_neighbor] = neighbor if eta[neighbor_of_neighbor] == neighbor else pivot_clustering[neighbor_of_neighbor]\n",
    "                        else:\n",
    "                            pivot_clustering[neighbor] = eta[neighbor] if eta[neighbor] == eta[eta[neighbor]] else neighbor\n",
    "                    # when eta[node] == neighbor. This neighbor may have been the pivot of node\n",
    "                    else:\n",
    "                        eta[node] = node\n",
    "                        for neighbor_of_neighbor in current_graph[node]:\n",
    "                            if pi[neighbor_of_neighbor] < pi[eta[node]]:\n",
    "                                eta[node] = neighbor_of_neighbor\n",
    "                        if eta[node] == node:\n",
    "                            pivot_clustering[node] = node\n",
    "                            for neighbor_of_neighbor in current_graph[node]:\n",
    "                                if neighbor_of_neighbor == node:\n",
    "                                    continue\n",
    "                                pivot_clustering[neighbor_of_neighbor] = node if eta[neighbor_of_neighbor] == node else pivot_clustering[neighbor_of_neighbor]\n",
    "                        else:\n",
    "                            pivot_clustering[node] = eta[node] if eta[node] == eta[eta[node]] else node\n",
    "                    dynamic_pivot_total_time += (time.time() - start_time)\n",
    "                    dynamic_pivot_worst_update_time = max(dynamic_pivot_worst_update_time, time.time() - start_time)\n",
    "\n",
    "                nodes_present_at_the_moment.remove(node)\n",
    "\n",
    "                del current_graph[node]\n",
    "            else:\n",
    "                deletion = False\n",
    "                try:\n",
    "                    # node addition\n",
    "                    node = next(random_node_iterator)\n",
    "                except StopIteration:\n",
    "                    # StopIteration is raised when the iterator is exhausted\n",
    "                    only_deletion = True\n",
    "                    continue\n",
    "                current_graph[node] = OptList()\n",
    "                nodes_present_at_the_moment.append(node)\n",
    "                # Add edges to previously arrived nodes\n",
    "                for neighbor in graph_adjacency_lists[node]:\n",
    "                    if neighbor in nodes_present_at_the_moment:\n",
    "                        # I have to update the pivot clustering\n",
    "                        start_time = time.time()\n",
    "                        # this is the case where node may become the new pivot of neighbor\n",
    "                        if pi[eta[neighbor]] > pi[node]:\n",
    "                            if eta[neighbor] == neighbor:\n",
    "                                for w in current_graph[neighbor]:\n",
    "                                    if w == neighbor:\n",
    "                                        continue\n",
    "                                    pivot_clustering[w] = w if eta[w]==neighbor else pivot_clustering[w]\n",
    "                            eta[neighbor] = node\n",
    "                            pivot_clustering[neighbor] = node if eta[node]==node else neighbor\n",
    "\n",
    "                        if pi[eta[node]] > pi[neighbor]:\n",
    "                            if eta[node] == node:\n",
    "                                for w in current_graph[node]:\n",
    "                                    if w == node:\n",
    "                                        continue\n",
    "                                    pivot_clustering[w] = w if eta[w]==node else pivot_clustering[w]\n",
    "                            eta[node] = neighbor\n",
    "                            pivot_clustering[node] = neighbor if eta[neighbor]==neighbor else node\n",
    "                        dynamic_pivot_total_time += (time.time() - start_time)\n",
    "                        dynamic_pivot_worst_update_time = max(dynamic_pivot_worst_update_time, time.time() - start_time)\n",
    "                        current_graph[neighbor].append(node)\n",
    "                        current_graph[node].append(neighbor)\n",
    "\n",
    "                # --------------------------------------Notify--------------------------------------\n",
    "                # ---------send Type 0 notifications-------------------\n",
    "            start_time = time.time()\n",
    "            notified_so_far = set()\n",
    "            if deletion:\n",
    "                for v in I_nodes[node]:\n",
    "                    if v == node:\n",
    "                        print(\"We may have a problem Houston\")\n",
    "                    B_nodes[v].remove(node)\n",
    "                got_type_0_notification = B_nodes[node].getRandom(notifications)\n",
    "                # we need to remove node, from any forward notification set\n",
    "                if not got_type_0_notification:\n",
    "                    continue\n",
    "                got_type_0_notification.discard(node)\n",
    "                if not got_type_0_notification:\n",
    "                    continue\n",
    "            else:\n",
    "                notified_so_far.add(node)\n",
    "                got_type_0_notification = current_graph[node].getRandom(notifications)\n",
    "                if got_type_0_notification:\n",
    "                    got_type_0_notification.discard(node)\n",
    "                    I_nodes[node].update(got_type_0_notification)\n",
    "                    for v in got_type_0_notification:\n",
    "                        B_nodes[v].append(node)\n",
    "\n",
    "            # ---------send Type 1 notifications-------------------\n",
    "            got_type_1_notification = set()\n",
    "            for v in got_type_0_notification:\n",
    "                if v not in current_graph:\n",
    "                    continue\n",
    "                # Avoid that the notification graph becomes too dense\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].remove(v)\n",
    "                # take another sample 1\n",
    "                br = current_graph[v].getRandom(notifications)\n",
    "                I_nodes[v].update(current_graph[v].getRandom(notifications))\n",
    "                I_nodes[v].discard(v)\n",
    "                I_nodes[v].discard(node)\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].append(v)\n",
    "                    if u not in notified_so_far:\n",
    "                        got_type_1_notification.add(u)\n",
    "                        notified_so_far.add(u)\n",
    "\n",
    "            # ---------send Type 2 notifications-------------------\n",
    "            got_type_2_notification = set()\n",
    "            for v in got_type_1_notification:\n",
    "                if v not in current_graph:\n",
    "                    continue\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].remove(v)\n",
    "                # take another sample 2\n",
    "                br = current_graph[v].getRandom(notifications)\n",
    "                I_nodes[v].update(current_graph[v].getRandom(notifications))\n",
    "                I_nodes[v].discard(v)\n",
    "                I_nodes[v].discard(node)\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].append(v)\n",
    "                    if (u not in notified_so_far) or (u not in got_type_2_notification) :\n",
    "                        got_type_2_notification.add(u)\n",
    "\n",
    "            # ---------receive Type 2 notifications-------------------\n",
    "            for v in got_type_2_notification:\n",
    "                if v not in current_graph:\n",
    "                    continue\n",
    "                # just update the sample\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].remove(v)\n",
    "                I_nodes[v] = current_graph[v].getRandom(notifications)\n",
    "                I_nodes[v].discard(v)\n",
    "                I_nodes[v].discard(node)\n",
    "                for u in I_nodes[v]:\n",
    "                    B_nodes[u].append(v)\n",
    "\n",
    "            # We check if it is a deletion or addition and initialize the respective\n",
    "            # variables\n",
    "            if not deletion:\n",
    "                sparse_graph[node] = OptList()\n",
    "                Phi_nodes[node] = set()\n",
    "            else:\n",
    "                Phi.discard(node)\n",
    "                neighbors_to_delete = [neighbor for neighbor in sparse_graph[node]]\n",
    "                for neighbor in neighbors_to_delete:\n",
    "                    sparse_graph[neighbor].remove(node)\n",
    "                    sparse_graph[node].remove(neighbor)\n",
    "                    Phi_nodes[neighbor].discard(node)\n",
    "                del sparse_graph[node]\n",
    "                del Phi_nodes[node]\n",
    "\n",
    "            # The interesting event set corresponds to the notified_so_far\n",
    "            for u in notified_so_far:\n",
    "                if deletion and node == u:\n",
    "                    continue\n",
    "                if u not in nodes_present_at_the_moment:\n",
    "                    continue\n",
    "                    print(\"Another problem\")\n",
    "                # if u is in Phi we first delete all its edges to nodes not in Phi\n",
    "                if u in Phi:\n",
    "                    neighbors_of_u = [v for v in sparse_graph[u]]\n",
    "                    for v in neighbors_of_u:\n",
    "                        if (u == v) or (v in Phi):\n",
    "                            continue\n",
    "                        sparse_graph[u].remove(v)\n",
    "                        sparse_graph[v].remove(u)\n",
    "                        Phi_nodes[v].discard(u)\n",
    "                Phi.discard(u)\n",
    "                # we now implement the Anchor procedure\n",
    "                anchor_prob = anchor_probability_numerator / len(current_graph[u])\n",
    "                if (random.random() < anchor_prob) and ProbAHeaviness(u, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                    Phi.add(u)\n",
    "                    for v in current_graph[u]:\n",
    "                        if ProbAgreement(u, v, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                            sparse_graph[u].append(v)\n",
    "                            sparse_graph[v].append(u)\n",
    "                            if u != v:\n",
    "                                Phi_nodes[v].add(u)\n",
    "\n",
    "                # Now we implement the Clean procedure\n",
    "                connected_nodes_in_anchor_set = [w for w in Phi_nodes[u]]\n",
    "                for w in connected_nodes_in_anchor_set:\n",
    "                    if w not in nodes_present_at_the_moment:\n",
    "                        Phi_nodes[u].discard(w)\n",
    "                        #del sparse_graph[w]\n",
    "                        continue\n",
    "                    if w == u:\n",
    "                        continue\n",
    "                    if ProbAgreement(u, w, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation) and ProbAHeaviness(w, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                        continue\n",
    "                    else:\n",
    "                        cleaning+=1\n",
    "                        sparse_graph[u].remove(w)\n",
    "                        sparse_graph[w].remove(u)\n",
    "                        Phi_nodes[u].discard(w)\n",
    "\n",
    "                # Now we implement the Connect procedure\n",
    "                J_u = current_graph[u].getRandom(num_samples_for_connect_procedure)\n",
    "                for w in J_u:\n",
    "                    phi_nodes_w = [r for r in Phi_nodes[w]]\n",
    "                    for r in phi_nodes_w:\n",
    "                        if r not in nodes_present_at_the_moment:\n",
    "                            Phi_nodes[w].discard(r)\n",
    "                            continue\n",
    "                        if r not in current_graph[u]:\n",
    "                            continue\n",
    "                        if ProbAgreement(u, r, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation) and ProbAHeaviness(r, current_graph, epsilon, num_samples_for_agreement_and_heavyness_calculation):\n",
    "                            sparse_graph[u].append(r)\n",
    "                            sparse_graph[r].append(u)\n",
    "                            Phi_nodes[u].add(r)\n",
    "            agreement_total_time+=(time.time() - start_time)\n",
    "            agreement_worst_update_time = max(agreement_worst_update_time, time.time() - start_time)\n",
    "            if deletion:\n",
    "                agreement_total_time_node_deletions+=(time.time() - start_time)\n",
    "                agreement_worst_update_time_node_deletions = max(agreement_worst_update_time_node_deletions, time.time() - start_time)\n",
    "            else:\n",
    "                agreement_total_time_node_additions+=(time.time() - start_time)\n",
    "                agreement_worst_update_time_node_additions = max(agreement_worst_update_time_node_additions, time.time() - start_time)\n",
    "            if agreement_worst_update_time > 20.5:\n",
    "                print(f\"Deletion = {deletion}\")\n",
    "            if (i+1) % 500 == 0:\n",
    "                clustering = connected_components(sparse_graph)\n",
    "                all_singletons = {element: index for index, element in enumerate(sparse_graph.keys())}\n",
    "                pivot_clustering_this_step = {u: pivot_clustering[u] for u in sparse_graph.keys()}\n",
    "\n",
    "                # Calculate correlation values and store them\n",
    "                corr_clustering = correlation_clustering_value(current_graph, clustering)\n",
    "                corr_all_singletons = correlation_clustering_value(current_graph, all_singletons)\n",
    "                corr_pivot_clustering = correlation_clustering_value(current_graph, pivot_clustering_this_step)\n",
    "                if corr_all_singletons != 0:\n",
    "                    correlation_values_clustering.append(corr_clustering/corr_all_singletons)\n",
    "                    correlation_values_all_singletons.append(corr_all_singletons/corr_all_singletons)\n",
    "                    correlation_values_pivot_clustering.append(corr_pivot_clustering/corr_all_singletons)\n",
    "    print(f\" agreement = {np.mean(correlation_values_clustering)} vs pivot = \"\n",
    "      f\"{np.mean(correlation_values_pivot_clustering)} vs singleton = {np.mean(correlation_values_all_singletons)}\")\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    print(f\" agreement = {np.mean(correlation_values_clustering)} vs pivot = \"\n",
    "      f\"{np.mean(correlation_values_pivot_clustering)} vs singleton = {np.mean(correlation_values_all_singletons)}\")                        \n",
    "    print(f\"agreement time = {agreement_total_time/float(num_experiments)} vs pivot total time = {dynamic_pivot_total_time/float(num_experiments)}\")\n",
    "    print(f\"agreement worst update time = {agreement_worst_update_time} vs pivot worst update time = {dynamic_pivot_worst_update_time}\")\n",
    "\n",
    "    print(f\"agreement time for additions = {agreement_total_time_node_additions/float(num_experiments)}\")\n",
    "    print(f\"agreement worst update time for additions = {agreement_worst_update_time_node_additions}\")\n",
    "\n",
    "    print(f\"agreement time for deletions = {agreement_total_time_node_deletions/float(num_experiments)}\")\n",
    "    print(f\"agreement worst update time for deletions = {agreement_worst_update_time_node_deletions}\")\n",
    "    \n",
    "    print(\"=========================================================================================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
